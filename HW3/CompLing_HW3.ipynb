{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание № 3. Исправление опечаток"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Доп. ранжирование по вероятности (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополните get_closest_hybrid_match в семинаре так, чтобы из кандадатов с одинаковым расстоянием редактирования выбиралось наиболее вероятное."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textdistance in c:\\users\\adugeen\\anaconda3\\lib\\site-packages (4.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install textdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "from nltk import sent_tokenize\n",
    "punctuation += \"«»—…“”\"\n",
    "punct = set(punctuation)\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from string import punctuation\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import textdistance\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_match_vec(text, X, vec, topn=20):\n",
    "    v = vec.transform([text])\n",
    "    \n",
    "    # вся эффективноть берется из того, что мы сразу считаем близость \n",
    "    # 1 вектора ко всей матрице (словам в словаре)\n",
    "    # считать по отдельности циклом было бы дольше\n",
    "    # вместо одного вектора может даже целая матрица\n",
    "    # тогда считаться в итоге будет ещё быстрее\n",
    "    \n",
    "    similarities = cosine_distances(v, X)[0]\n",
    "    topn = similarities.argsort()[:topn] \n",
    "    \n",
    "    return [(id2word[top], similarities[top]) for top in topn]\n",
    "\n",
    "def get_closest_match_with_metric(text, lookup,topn=20, metric=textdistance.levenshtein):\n",
    "    # Counter можно использовать и с не целыми числами\n",
    "    similarities = Counter()\n",
    "    \n",
    "    for word in lookup:\n",
    "        similarities[word] = metric.normalized_similarity(text, word) \n",
    "    \n",
    "    return similarities.most_common(topn)\n",
    "\n",
    "def get_closest_hybrid_match(text, X, vec, topn=3, metric=textdistance.damerau_levenshtein):\n",
    "    candidates = get_closest_match_vec(text, X, vec, topn*4)\n",
    "    lookup = [cand[0] for cand in candidates]\n",
    "    closest = get_closest_match_with_metric(text, lookup, topn, metric=metric)\n",
    "\n",
    "    max_closest_prior = max(closest, key=lambda x: x[1])[1]\n",
    "    closest_candidates = [(cand[0], P(cand[0])) for cand in closest if cand[1] == max_closest_prior]\n",
    "    \n",
    "    return max(closest_candidates, key=lambda x: x[1])[0]\n",
    "\n",
    "def predict_mistaken(word, vocab):\n",
    "    return 0 if word in vocab else 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open('data\\wiki_data.txt', encoding='utf8').read()\n",
    "vocab = Counter(re.findall('\\w+', corpus.lower()))\n",
    "\n",
    "word2id = list(vocab.keys())\n",
    "id2word = {i:word for i, word in enumerate(vocab)}\n",
    "\n",
    "\n",
    "vec = CountVectorizer(analyzer='char', ngram_range=(1,3), max_features=1000)\n",
    "X = vec.fit_transform(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = sum(vocab.values())\n",
    "\n",
    "def P(word, N=N):\n",
    "    return vocab[word] / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'жить'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_closest_hybrid_match('жзть', X, vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Суть доработки состоит в том, что из списка с кандидатами с расстоянием редактирования получается максимальное значение. По нему происходит фильтрация кандидатов. Затем у каждого кандидата вычисляется вероятность встречи в корпусе и среди них после выбирается тот кандидат, который имеет наибольшую вероятность."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Symspell (5 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте алгоритм Symspell. Он похож на алгоритм Норвига, но проще и быстрее. Там к словам применяется только одна операция - удаление символа. Описание алгоритма по шагам:\n",
    "\n",
    "1) Составляется словарь правильных слов  \n",
    "2) На основе словаря правильных слов составляется словарь удалений - для каждого правильного слова создаются все варианты удалений и создается словарь, где ключ - слово с удалением, а значение - правильное слово   \n",
    "3) Для выбора исправления для слова с опечаткой генерируются все варианты удаления, из них выбираются те, что есть в словаре удалений, построенного на шаге 2. Слово с опечаткой заменяется на правильное слово, соответствующее варианту удаления  \n",
    "4) Если в словаре удалений есть несколько вариантов, то выбирается удаление, которому соответствует наиболее вероятное правильное слово  \n",
    "\n",
    "\n",
    "Оцените качество полученного алгоритма теми же тремя метриками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS = list(vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deletes(word):\n",
    "    deletes = [word[:i] + word[i + 1:]  for i in range(len(word))]\n",
    "    deletes.append(word)\n",
    "    return deletes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def known(words):\n",
    "    return set(w for w in words if w in WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidates(word):\n",
    "    return (known([word]) or known(correction_variants(word)) or [word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correction(word):\n",
    "    return max(candidates(word), key=P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correction_variants(word):\n",
    "    errors = get_deletes(word)\n",
    "    \n",
    "    set_corr = set()\n",
    "\n",
    "    for error in errors:\n",
    "        set_corr = set_corr or set(dict_corr.get(error, ''))\n",
    "    \n",
    "    return set_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_mistakes():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    total_mistaken = 0\n",
    "    mistaken_fixed = 0\n",
    "\n",
    "    total_correct = 0\n",
    "    correct_broken = 0\n",
    "\n",
    "    cashed = {}\n",
    "    for i in range(len(true)):\n",
    "        word_pairs = align_words(true[i], bad[i])\n",
    "        for pair in word_pairs:\n",
    "            # чтобы два раза не исправлять одно и тоже слово - закешируем его\n",
    "            # перед тем как считать исправление проверим нет ли его в кеше\n",
    "\n",
    "            predicted = cashed.get(pair[1], correction(pair[1]))\n",
    "            cashed[pair[1]] = predicted\n",
    "\n",
    "\n",
    "            if predicted == pair[0]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "            if pair[0] == pair[1]:\n",
    "                total_correct += 1\n",
    "                if pair[0] !=  predicted:\n",
    "                    correct_broken += 1\n",
    "            else:\n",
    "                total_mistaken += 1\n",
    "                if pair[0] == predicted:\n",
    "                    mistaken_fixed += 1\n",
    "\n",
    "        if not i % 100:\n",
    "            print(i)\n",
    "            \n",
    "    print(correct/total)\n",
    "    print(mistaken_fixed/total_mistaken)\n",
    "    print(correct_broken/total_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_words(sent_1, sent_2):\n",
    "    tokens_1 = sent_1.lower().split()\n",
    "    tokens_2 = sent_2.lower().split()\n",
    "    \n",
    "    tokens_1 = [token.strip(punctuation) for token in tokens_1]\n",
    "    tokens_2 = [token.strip(punctuation) for token in tokens_2]\n",
    "    \n",
    "    tokens_1 = [token for token in tokens_1 if token]\n",
    "    tokens_2 = [token for token in tokens_2 if token]\n",
    "    \n",
    "    assert len(tokens_1) == len(tokens_2)\n",
    "    \n",
    "    return list(zip(tokens_1, tokens_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_corr = {}\n",
    "\n",
    "for word in WORDS:\n",
    "    if len(word) > 1:\n",
    "        deletes = get_deletes(word)\n",
    "        for delete in deletes:\n",
    "            if delete in dict_corr:\n",
    "                dict_corr[delete].append(word)\n",
    "            else:\n",
    "                dict_corr[delete] = [word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = open('data\\sents_with_mistakes.txt', encoding='utf8').read().splitlines()\n",
    "true = open('data\\correct_sents.txt', encoding='utf8').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0.8680340170085042\n",
      "0.327639751552795\n",
      "0.05202710462845986\n"
     ]
    }
   ],
   "source": [
    "eval_mistakes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistakes = []\n",
    "total = 0\n",
    "for i in range(len(true)):\n",
    "    word_pairs = align_words(true[i], bad[i])\n",
    "    \n",
    "    \n",
    "    for pair in word_pairs:\n",
    "        if pair[0] != pair[1]:\n",
    "            mistakes.append(pair)\n",
    "        total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('сегодня', 'седня', 'ясеня'),\n",
       " ('вообще', 'вобще', 'общей'),\n",
       " ('вообще', 'ваще', 'чаще'),\n",
       " ('естественно', 'естесственно', 'естественно'),\n",
       " ('хочется', 'хочеться', 'хочется'),\n",
       " ('кстати', 'кстате', 'статей'),\n",
       " ('очень', 'ооочень', 'ооочень'),\n",
       " ('как-то', 'както', 'актов'),\n",
       " ('очень', 'оооочень', 'оооочень'),\n",
       " ('это', 'ето', 'что')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(wt[0], wt[1], correction(wt[1])) for wt, _ in Counter(mistakes).most_common(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 26 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'солнце'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "correction('солнвце')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'насмехатьсяаававттававаываываы'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "correction('насмехатьсяаававттававаываываы')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По качеству работы алгоритм Symspell уступает алгоритму Норвига, хотя работает он  не сильно хуже. Главным же преимуществом является его скорость работы из-за использования только одной операции удаления."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *3. Настройка гиперпараметров. (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У метода из первого заданий много гиперпараметров (те которые нужно подбирать самостоятельно). Это параметры векторайзера, topn, метрика редактирования. Поэкспериментируйте с ними. \n",
    "\n",
    "Проведите как минимум 10 экспериментов с разными параметрами. Для каждого эксперимента укажите мотивацию (например, \"слишком маленький topn в get_closest_match_vec приводит к тому, что некоторые хорошие варианты не доходят до get_closest_match_with_metric, попробуем его увеличить\")\n",
    "\n",
    "Старайтесь получить улучшение, но если улучшить не получится, то это не страшно. Главное, чтобы эксперименты были осмысленными, а не рандомными. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_mistakes_closest(X, vec, topn=3, metric=textdistance.damerau_levenshtein):\n",
    "    mistakes = []\n",
    "    total_mistaken = 0\n",
    "    mistaken_fixed = 0\n",
    "\n",
    "    total_correct = 0\n",
    "    correct_broken = 0\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    cashed = {}\n",
    "    for i in range(len(true)):\n",
    "        word_pairs = align_words(true[i], bad[i])\n",
    "        for pair in word_pairs:\n",
    "            if predict_mistaken(pair[1], vocab):\n",
    "                pred = cashed.get(pair[1], get_closest_hybrid_match(pair[1], X, vec, topn=topn, metric=metric))\n",
    "                cashed[pair[1]] = pred\n",
    "            else:\n",
    "                pred = pair[1]\n",
    "\n",
    "\n",
    "            if pred == pair[0]:\n",
    "                correct += 1\n",
    "            else:\n",
    "                mistakes.append((pair[0], pair[1], pred))\n",
    "            total += 1\n",
    "\n",
    "            if pair[0] == pair[1]:\n",
    "                total_correct += 1\n",
    "                if pair[0] != pred:\n",
    "                    correct_broken += 1\n",
    "            else:\n",
    "                total_mistaken += 1\n",
    "                if pair[0] == pred:\n",
    "                    mistaken_fixed += 1\n",
    "\n",
    "        if not i % 100:\n",
    "            print(i)\n",
    "    \n",
    "    print(correct/total)\n",
    "    print(mistaken_fixed/total_mistaken)\n",
    "    print(correct_broken/total_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0.8556278139069535\n",
      "0.48835403726708076\n",
      "0.09004249454461927\n",
      "Wall time: 8min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#base\n",
    "\n",
    "vec = CountVectorizer(analyzer='char', ngram_range=(1,3), max_features=1000, )\n",
    "X = vec.fit_transform(vocab)\n",
    "\n",
    "eval_mistakes_closest(X, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0.8556278139069535\n",
      "0.48835403726708076\n",
      "0.09004249454461927\n",
      "Wall time: 7min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Имеет смысл увеличить параметр н-грамм, \n",
    "# тем самым увеличится количество возможных ошибок на слов и возможно увеличится качество.\n",
    "\n",
    "vec = CountVectorizer(analyzer='char', ngram_range=(1,4), max_features=1000, )\n",
    "X = vec.fit_transform(vocab)\n",
    "\n",
    "eval_mistakes_closest(X, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0.8544272136068034\n",
      "0.47903726708074534\n",
      "0.09004249454461927\n",
      "Wall time: 6min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Попробуем уменьшить количество фич, чтобы сократить время работы, вероятно это никак не повлияет на качество,\n",
    "# потому что 300 фич может быть достаточно для обучения.\n",
    "\n",
    "vec = CountVectorizer(analyzer='char', ngram_range=(1,4), max_features=300, )\n",
    "X = vec.fit_transform(vocab)\n",
    "\n",
    "eval_mistakes_closest(X, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0.8577288644322161\n",
      "0.5046583850931677\n",
      "0.09004249454461927\n",
      "Wall time: 6min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Попробуем увеличить значение параметра topn, возможно в сформированном списке косинусного расстояния найдётся лучший кандидат.\n",
    "\n",
    "vec = CountVectorizer(analyzer='char', ngram_range=(1,4), max_features=300, )\n",
    "X = vec.fit_transform(vocab)\n",
    "\n",
    "eval_mistakes_closest(X, vec, topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0.856328164082041\n",
      "0.4937888198757764\n",
      "0.09004249454461927\n",
      "Wall time: 6min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Попробуем поменять метрику на метрику Левенштейна, поскольку в ней происходит меньше операций, чем у Левенштейна-Дамерау,\n",
    "# потому, вероятно, алгоритм будет работать быстрее.\n",
    "\n",
    "vec = CountVectorizer(analyzer='char', ngram_range=(1,4), max_features=300, )\n",
    "X = vec.fit_transform(vocab)\n",
    "\n",
    "eval_mistakes_closest(X, vec, topn=15, metric=textdistance.levenshtein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0.8554277138569285\n",
      "0.48680124223602483\n",
      "0.09004249454461927\n",
      "Wall time: 4min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Попробуем снизить метрику max_df, потому что она может пропускать важные фичи, \n",
    "# а поскольку мы работаем с символами, то это может оказаться критично.\n",
    "\n",
    "vec = CountVectorizer(analyzer='char', ngram_range=(1,4), max_features=300, max_df=0.3, )\n",
    "X = vec.fit_transform(vocab)\n",
    "\n",
    "eval_mistakes_closest(X, vec, topn=15, metric=textdistance.levenshtein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0.8498249124562282\n",
      "0.44332298136645965\n",
      "0.09004249454461927\n",
      "Wall time: 3min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Попробуем установить метрику min_df в качестве 0.05, потому что такое значение будет игнорировать те фичи, которые будут\n",
    "# специфичны (например, какие-то специальные символы).\n",
    "\n",
    "vec = CountVectorizer(analyzer='char', ngram_range=(1,4), max_features=300, max_df=0.3, min_df=0.05)\n",
    "X = vec.fit_transform(vocab)\n",
    "\n",
    "eval_mistakes_closest(X, vec, topn=15, metric=textdistance.levenshtein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0.8545272636318159\n",
      "0.4798136645962733\n",
      "0.09004249454461927\n",
      "Wall time: 3min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Попробуем поменять параметр 'analyzer' на 'char_wb', потому что, исходя из документации, \n",
    "# будут создаваться n-граммы внутри слова, что, возможно, может снизить частоту каких-нибудь специальных символов. \n",
    "\n",
    "vec = CountVectorizer(analyzer='char_wb', ngram_range=(1,4), max_features=300, max_df=0.3, min_df=0.05)\n",
    "X = vec.fit_transform(vocab)\n",
    "\n",
    "eval_mistakes_closest(X, vec, topn=15, metric=textdistance.levenshtein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0.8534267133566783\n",
      "0.47127329192546585\n",
      "0.09004249454461927\n",
      "Wall time: 3min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Имеет смысл попробовать другой векторайзер - TfidfVectorizer, поскольку в предыдущей лабораторной он показал себя лучше.\n",
    "\n",
    "vec = TfidfVectorizer(analyzer='char_wb', ngram_range=(1,4), max_features=300, max_df=0.3, min_df=0.05)\n",
    "X = vec.fit_transform(vocab)\n",
    "\n",
    "eval_mistakes_closest(X, vec, topn=15, metric=textdistance.levenshtein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0.8562281140570285\n",
      "0.49301242236024845\n",
      "0.09004249454461927\n",
      "Wall time: 9min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Похоже, что TfidfVectorizer работает хуже, некоторые параметры не дают прироста качества и скорости. \n",
    "# Попробуем взять те параметры, которые улучшали модель, и те, с которых начинался бейзлайн. \n",
    "# В частности, можно дальше попробовать увеличить topn,\n",
    "# потому что гипотеза оказалась верна и некоторые исправления также могут не доходить.\n",
    "# Также изменение метрики помогло увеличить скорость работы алгоритма.\n",
    "\n",
    "vec = CountVectorizer(analyzer='char_wb', ngram_range=(1,3), max_features=1000, )\n",
    "X = vec.fit_transform(vocab)\n",
    "\n",
    "eval_mistakes_closest(X, vec, topn=25, metric=textdistance.levenshtein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0.8566283141570785\n",
      "0.49611801242236025\n",
      "0.09004249454461927\n",
      "Wall time: 9min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Предположим, что нам не важна частотность у векторайзера, поскольку будет достаточно знать, \n",
    "# есть ли символ или n-грамма в слове, и результат косинусного расстояния может оказаться лучше. \n",
    "# В таком случае надо изменить параметр binary у векторайзера на True. \n",
    "\n",
    "vec = CountVectorizer(analyzer='char_wb', ngram_range=(1,3), max_features=1000, binary=True)\n",
    "X = vec.fit_transform(vocab)\n",
    "\n",
    "eval_mistakes_closest(X, vec, topn=25, metric=textdistance.levenshtein)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате получилось улучшить качество работы алгоритма, но это улучшение по факту не совсем значительное, примерно на сотые или тысячные доли. Ещё при некоторых запусках получилось увеличить скорость работы, но иногда алгоритм работает и дольше стартовых параметров, скорее всего это зависит от нагруженности процессора. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
