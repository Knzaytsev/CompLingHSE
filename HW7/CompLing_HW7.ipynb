{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0ca41e2"
      },
      "source": [
        "# Домашнее задание № 7"
      ],
      "id": "d0ca41e2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f0588a6"
      },
      "source": [
        "### Задание 1 Реализовать алгоритм Леска и проверить его на реальном датасете (8 баллов)"
      ],
      "id": "6f0588a6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6a5283f"
      },
      "source": [
        "Ворднет можно использовать для дизамбигуации. Самый простой алгоритм дизамбигуации - алгоритм Леска. В нём нужное значение слова находится через пересечение слов контекста, в котором употреблено это слово, с определениями значений слова из ворднета. Значение с максимальным пересечением - нужное."
      ],
      "id": "d6a5283f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6f082cd"
      },
      "source": [
        "Реализуйте его"
      ],
      "id": "f6f082cd"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Au-t5V5l9_0m",
        "outputId": "b11134b9-ff98-4581-ee4d-846d608aa83a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.6 MB/s \n",
            "\u001b[?25hCollecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 33.8 MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.6.2)\n",
            "Installing collected packages: pymorphy2-dicts-ru, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n",
            "Collecting razdel\n",
            "  Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: razdel\n",
            "Successfully installed razdel-0.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pymorphy2\n",
        "!pip install razdel"
      ],
      "id": "Au-t5V5l9_0m"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dr6zZH8499lN",
        "outputId": "7cb185a4-56c6-445c-9f28-278693abb8bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "id": "Dr6zZH8499lN"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uosKqdmn9pt6"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet as wn\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "from string import punctuation \n",
        "from razdel import tokenize as razdel_tokenize\n",
        "import spacy\n",
        "\n",
        "\n",
        "punct = punctuation+'«»—…“”*№–'\n",
        "stops = set(stopwords.words('russian'))\n",
        "morph = MorphAnalyzer()\n",
        "nlp = spacy.load('en', disable=['parser', 'ner'])"
      ],
      "id": "uosKqdmn9pt6"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "D8sXZaRX-mvF"
      },
      "outputs": [],
      "source": [
        "def normalize(text):\n",
        "    \n",
        "    doc = nlp(text)\n",
        "    \n",
        "    words = [token.lemma_ for token in doc if not token.is_punct]\n",
        "\n",
        "    return words"
      ],
      "id": "D8sXZaRX-mvF"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bEz9oYV3_Rbm"
      },
      "outputs": [],
      "source": [
        "def compute_overlap(signature, context):\n",
        "  overlap = set(signature) & set(context)\n",
        "  return len(overlap)"
      ],
      "id": "bEz9oYV3_Rbm"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "668e6803"
      },
      "outputs": [],
      "source": [
        "def lesk(word, sentence):\n",
        "  \"\"\"Ваш код тут\"\"\"\n",
        "  bestsense = 0\n",
        "  maxoverlap = 0\n",
        "  \n",
        "  context = normalize(sentence)\n",
        "  if word in context:\n",
        "    context = context[:context.index(word)] + context[context.index(word)+1:]\n",
        "\n",
        "  for i, syns in enumerate(wn.synsets(word)):\n",
        "    # ваш код \n",
        "    signature = normalize(syns.definition())\n",
        "    overlap = compute_overlap(signature, context)\n",
        "    \n",
        "    if overlap > maxoverlap:\n",
        "      maxoverlap = overlap\n",
        "      bestsense = i\n",
        "\n",
        "  return bestsense\n"
      ],
      "id": "668e6803"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1a4309c"
      },
      "source": [
        "**Проверьте насколько хорошо работает такой метод на датасете из семинара**"
      ],
      "id": "d1a4309c"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1aa7ae8f"
      },
      "outputs": [],
      "source": [
        "corpus_wsd = []\n",
        "corpus = open('corpus_wsd_50k.txt').read().split('\\n\\n')\n",
        "for sent in corpus:\n",
        "    corpus_wsd.append([s.split('\\t') for s in sent.split('\\n')])"
      ],
      "id": "1aa7ae8f"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Lw6AoaKoHoXX"
      },
      "outputs": [],
      "source": [
        "disambig_corpus = []\n",
        "\n",
        "for sentence in corpus_wsd:\n",
        "  for word in sentence:\n",
        "    if word[0] != '':\n",
        "      disambig_corpus.append(word)"
      ],
      "id": "Lw6AoaKoHoXX"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "p9vqSw6fLzp9"
      },
      "outputs": [],
      "source": [
        "sentences = []\n",
        "\n",
        "for sent in corpus:\n",
        "  cnt = 0\n",
        "  for word in sent.split('\\n'):\n",
        "\n",
        "    if word.split('\\t')[0] != '':\n",
        "      cnt += 1\n",
        "  \n",
        "  for i in range(cnt):\n",
        "    sentences.append(' '.join([s.split('\\t')[2] for s in sent.split('\\n')]))"
      ],
      "id": "p9vqSw6fLzp9"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YdqWqgqPCMLU"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ],
      "id": "YdqWqgqPCMLU"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ydEF61mWI7U7"
      },
      "outputs": [],
      "source": [
        "result = [wn.synsets(word[1])[lesk(word[1], sentence)] == wn.lemma_from_key(word[0]).synset() for sentence, word in zip(sentences[:10000], disambig_corpus[:10000])]"
      ],
      "id": "ydEF61mWI7U7"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "us6WBHiXTM0Z",
        "outputId": "671124fc-c8a2-42b0-9e2d-52f8d9ca2713"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3737"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "result.count(True)/len(result)"
      ],
      "id": "us6WBHiXTM0Z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как можно видеть, результат работы алгоритма не совсем хороший. В ходе написания алгоритма было обнаружено, что лемматизация очень сильно влияет на качество алгоритма. Из всех библиотек, на мой взгляд, лучшим образом показал себя SpaCy."
      ],
      "metadata": {
        "id": "kkDhcUSmV-aL"
      },
      "id": "kkDhcUSmV-aL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c48b45d4"
      },
      "source": [
        "### Задание 2* (2 балла)"
      ],
      "id": "c48b45d4"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1c17e25c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef4ddb34-073a-493f-ba9f-39138bb9c590"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1DIRIYCkUBSk6lb0RTQhF7bTs-lKKkPSF\n",
            "To: /content/all.a010.p10.d300.w5.m100.nonorm.slim.joblib\n",
            "100% 1.46G/1.46G [00:12<00:00, 121MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown --id 1DIRIYCkUBSk6lb0RTQhF7bTs-lKKkPSF"
      ],
      "id": "1c17e25c"
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/nlpub/russe-wsi-kit/master/data/main/wiki-wiki/train.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2bgsC3uLbLZ",
        "outputId": "cbe1d7fc-7bd1-4269-f167-d5e3eaf2c117"
      },
      "id": "C2bgsC3uLbLZ",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-04 15:20:48--  https://raw.githubusercontent.com/nlpub/russe-wsi-kit/master/data/main/wiki-wiki/train.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 365614 (357K) [text/plain]\n",
            "Saving to: ‘train.csv’\n",
            "\n",
            "\rtrain.csv             0%[                    ]       0  --.-KB/s               \rtrain.csv           100%[===================>] 357.04K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2022-02-04 15:20:48 (92.1 MB/s) - ‘train.csv’ saved [365614/365614]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/lopuhin/python-adagram.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42LFBAkyMBO2",
        "outputId": "a0433ced-6723-4299-c052-4806090bbc2c"
      },
      "id": "42LFBAkyMBO2",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/lopuhin/python-adagram.git\n",
            "  Cloning https://github.com/lopuhin/python-adagram.git to /tmp/pip-req-build-wc76_19q\n",
            "  Running command git clone -q https://github.com/lopuhin/python-adagram.git /tmp/pip-req-build-wc76_19q\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from adagram==0.0.1) (0.29.26)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from adagram==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.9 in /usr/local/lib/python3.7/dist-packages (from adagram==0.0.1) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from adagram==0.0.1) (1.15.0)\n",
            "Building wheels for collected packages: adagram\n",
            "  Building wheel for adagram (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for adagram: filename=adagram-0.0.1-cp37-cp37m-linux_x86_64.whl size=450487 sha256=419b6b1948a77d9b8b090e6315804ab4e93eb3ca57865a229bee434b83bcf840\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_a83siiy/wheels/5a/8c/f9/7dee902dd325a3317e768f126aa6f7aa085c79a6e763ed2cb8\n",
            "Successfully built adagram\n",
            "Installing collected packages: adagram\n",
            "Successfully installed adagram-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import adagram\n",
        "import numpy as np\n",
        "from sklearn.metrics import adjusted_rand_score"
      ],
      "metadata": {
        "id": "8qRvTqtFLkoj"
      },
      "id": "8qRvTqtFLkoj",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('train.csv', sep='\\t')"
      ],
      "metadata": {
        "id": "hqc9OP29Lfnx"
      },
      "id": "hqc9OP29Lfnx",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding_adagram(text, model, window, dim):\n",
        "    \n",
        "    \n",
        "    word2context = []\n",
        "    for i in range(len(text)-1):\n",
        "        left = max(0, i-window)\n",
        "        word = text[i]\n",
        "        left_context = text[left:i]\n",
        "        right_context = text[i+1:i+window]\n",
        "        context = left_context + right_context\n",
        "        word2context.append((word, context))\n",
        "    \n",
        "    \n",
        "    \n",
        "    vectors = np.zeros((len(word2context), dim))\n",
        "    \n",
        "    for i,word in enumerate(word2context):\n",
        "        word, context = word\n",
        "        try:\n",
        "            sense = model.disambiguate(word, context).argmax()\n",
        "            v = model.sense_vector(word, sense)\n",
        "            vectors[i] = v \n",
        "\n",
        "        except (KeyError):\n",
        "            continue\n",
        "    \n",
        "    if vectors.any():\n",
        "        vector = np.average(vectors, axis=0)\n",
        "    else:\n",
        "        vector = np.zeros((dim))\n",
        "    \n",
        "    return vector"
      ],
      "metadata": {
        "id": "0mjVsgjwLuDv"
      },
      "id": "0mjVsgjwLuDv",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_rus(text):\n",
        "    \n",
        "    words = [token.text.strip(punct) for token in list(razdel_tokenize(text))]\n",
        "    words = [morph.parse(word)[0].normal_form for word in words if word and word not in stops]\n",
        "\n",
        "    return words"
      ],
      "metadata": {
        "id": "scM41a_2M9b6"
      },
      "id": "scM41a_2M9b6",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_df = df.groupby('word')[['word', 'context', 'gold_sense_id']]"
      ],
      "metadata": {
        "id": "uGAmntzbL1Xy"
      },
      "id": "uGAmntzbL1Xy",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vm = adagram.VectorModel.load('all.a010.p10.d300.w5.m100.nonorm.slim.joblib')"
      ],
      "metadata": {
        "id": "xbU_FnmFNvSv"
      },
      "id": "xbU_FnmFNvSv",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dim = 300\n",
        "window = 5"
      ],
      "metadata": {
        "id": "8MBeiXLoN92q"
      },
      "id": "8MBeiXLoN92q",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import *"
      ],
      "metadata": {
        "id": "OSzYBqIOOTlg"
      },
      "id": "OSzYBqIOOTlg",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = [AffinityPropagation(damping=0.7, max_iter=100, verbose=True), KMeans(3, tol=0.001, max_iter=350), DBSCAN(min_samples=1, eps=0.1,leaf_size=25)]\n",
        "ARI_by_model = {type(model).__name__: [] for model in models}\n",
        "\n",
        "for key, _ in grouped_df:\n",
        "    texts = grouped_df.get_group(key)['context'].apply(normalize_rus)\n",
        "    X = np.zeros((len(texts), dim))\n",
        "    \n",
        "    for i, text in enumerate(texts):\n",
        "        text = [word for word in text if word != key]\n",
        "        X[i] = get_embedding_adagram(text, vm, window, dim)\n",
        "        \n",
        "    for model in models:\n",
        "      model.fit(X)\n",
        "      labels = np.array(model.labels_)+1\n",
        "      ARI_by_model[type(model).__name__].append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5Qz-A4mMlmh",
        "outputId": "ada05e20-4b49-4c19-89c0-0103a86f0a33"
      },
      "id": "P5Qz-A4mMlmh",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/adagram/model.py:171: RuntimeWarning: divide by zero encountered in log\n",
            "  z = np.log(z)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged after 32 iterations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/adagram/model.py:171: RuntimeWarning: divide by zero encountered in log\n",
            "  z = np.log(z)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Did not converge\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/cluster/_affinity_propagation.py:253: ConvergenceWarning: Affinity propagation did not converge, this model will not have any cluster centers.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/adagram/model.py:171: RuntimeWarning: divide by zero encountered in log\n",
            "  z = np.log(z)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Did not converge\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/cluster/_affinity_propagation.py:253: ConvergenceWarning: Affinity propagation did not converge, this model will not have any cluster centers.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/adagram/model.py:171: RuntimeWarning: divide by zero encountered in log\n",
            "  z = np.log(z)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged after 63 iterations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "{name: np.mean(val) for name, val in ARI_by_model.items()}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBR4wNZsWx6P",
        "outputId": "dfee82c5-2bfe-4876-eccb-cb14dc4f325a"
      },
      "id": "jBR4wNZsWx6P",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'AffinityPropagation': 0.06708180628651,\n",
              " 'DBSCAN': 0.0010826021180129009,\n",
              " 'KMeans': 0.3570428185789412}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Среди всех алгоритмов лучшее качество достигается KMeans. Причём при каждом запуске его результаты могут сильно разниться."
      ],
      "metadata": {
        "id": "f2F2aeKTZAwi"
      },
      "id": "f2F2aeKTZAwi"
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "CompLing_HW7",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}